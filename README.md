# ğŸ¤– Monit-Intel Agent

A **LangGraph + Llama 3.1** powered agent that monitors server health via Monit, analyzes logs intelligently, and performs automated root-cause analysis. Features an interactive MU/TH/UR chat interface for querying system health and executing safe remediation actions.

**Quick links:**
- ğŸ“– [Install & Setup](#-install--setup)
- ğŸš€ [Quick Start](#-quick-start)
- ğŸ’¬ [Usage](#-usage)
- ğŸ—ï¸ [Architecture](#-architecture)
- âš™ï¸ [Configuration](#-configuration)
- ğŸ› [Troubleshooting](#-troubleshooting)

---

## ğŸ“– Install & Setup

### Prerequisites

- **Pixi** (lightweight Conda alternative) - [Install here](https://pixi.sh)
- **Monit** running on `localhost:2812` with XML API enabled
- **Ollama** running Llama 3.1:8b on GPU (RTX 4000 or similar)
- **Linux system** (tested on Ubuntu 24.04)

### Step 1: Clone & Install Dependencies

```bash
cd /home/heverz/py_projects/monit-intel
pixi install
```

### Step 2: Configure Credentials

**Option A: Development (Local .env - Easy)**
```bash
cat > .env << EOF
MONIT_USER=admin
MONIT_PASS=monit
MONIT_URL=http://localhost:2812/_status?format=xml
EOF
```

**Option B: Production (Systemd - Secure & Recommended)**

Credentials stored securely in systemd drop-in files (not in git, not in project folder):

```bash
# Create environment files
sudo mkdir -p /etc/systemd/system/monit-intel-{agent,ingest}.service.d/

# Agent environment
sudo tee /etc/systemd/system/monit-intel-agent.service.d/env.conf > /dev/null << EOF
[Service]
Environment="MONIT_USER=admin"
Environment="MONIT_PASS=your_monit_password"
Environment="MONIT_URL=http://localhost:2812/_status?format=xml"
EOF

# Ingest environment
sudo tee /etc/systemd/system/monit-intel-ingest.service.d/env.conf > /dev/null << EOF
[Service]
Environment="MONIT_USER=admin"
Environment="MONIT_PASS=your_monit_password"
Environment="MONIT_URL=http://localhost:2812/_status?format=xml"
EOF

# Lock down permissions
sudo chmod 600 /etc/systemd/system/monit-intel-*/service.d/env.conf

# Reload systemd
sudo systemctl daemon-reload
```

**Benefits of Option B:**
- âœ… Credentials NOT in git repo
- âœ… Credentials NOT in project folder
- âœ… Only readable by root (chmod 600)
- âœ… Easy rotation without code redeploy

### Step 3: Set Up Chat Credentials

Chat authentication is **separate from Monit credentials**. Initialize the chat UI login credentials:

```bash
cd /home/heverz/py_projects/monit-intel

# Set your chat UI username and password
pixi run python -m monit_intel.chat_auth your_username your_secure_password

# Example:
pixi run python -m monit_intel.chat_auth admin MySecurePassword123

# Check status
pixi run python << 'EOF'
from monit_intel.chat_auth import get_chat_credentials_status
status = get_chat_credentials_status()
print(f"Chat credentials configured: {status['configured']}")
print(f"Credentials count: {status['count']}")
EOF
```

**Note:** You can have different passwords for:
- **Monit API** (in systemd env files or .env)
- **Chat UI** (stored securely in SQLite with password hashing)

### Step 4: Verify Setup

```bash
# Test Monit connection (using Monit credentials from systemd env)
curl -u $(echo $MONIT_USER:$MONIT_PASS) http://localhost:2812/_status?format=xml | head -20

# Test Ollama (should return model info)
curl http://localhost:11434/api/tags | jq .

# Test database (should show >0 snapshots)
pixi run python << 'EOF'
import sqlite3
conn = sqlite3.connect("monit_history.db")
cursor = conn.cursor()
cursor.execute("SELECT COUNT(*) FROM snapshots")
print(f"âœ“ Database ready: {cursor.fetchone()[0]} snapshots")
cursor.execute("SELECT COUNT(*) FROM chat_credentials")
print(f"âœ“ Chat credentials: {cursor.fetchone()[0]} user(s) configured")
conn.close()
EOF
```

---

## ğŸš€ Quick Start

### Start the Services (Development)

**Terminal 1: Start the agent with API**
```bash
cd /home/heverz/py_projects/monit-intel

# Use your Monit credentials from systemd env or .env
MONIT_USER=admin MONIT_PASS=your_monit_password pixi run agent
```

You should see:
```
INFO:     Uvicorn running on http://0.0.0.0:8000
INFO:     Application startup complete
```

**Terminal 2: Start the ingestion (optional - runs automatically every 5 min in production)**
```bash
cd /home/heverz/py_projects/monit-intel
pixi run ingest
```

### Access the Chat UI

Open your browser:
```
http://localhost:8000/chat
```

Login with the **chat credentials** you set up in Step 3:
- **Username:** Your chosen username
- **Password:** Your chosen password

**Note:** These are **different from your Monit service password**. If you set them up as:
```bash
pixi run python -m monit_intel.chat_auth admin your_secure_password
```

Then login with:
- Username: `admin`
- Password: `your_secure_password`

### Start the Services (Production with Systemd)

```bash
# Install systemd services (one-time setup)
sudo bash /home/heverz/py_projects/monit-intel/config/systemd/install-services.sh

# Start agent
sudo systemctl start monit-intel-agent.service
sudo systemctl status monit-intel-agent.service

# Start ingest timer
sudo systemctl start monit-intel-ingest.timer
systemctl list-timers monit-intel-ingest.timer

# View logs
journalctl -u monit-intel-agent.service -f
```

---

## ğŸ’¬ Usage

### 1. Web Chat UI (Interactive & Easiest)

**Start the agent first:**
```bash
MONIT_USER=admin MONIT_PASS=your_monit_password pixi run agent
```

**Open in browser:**
```
http://localhost:8000/chat
```

**Login with your chat credentials** (the ones you configured in setup step 3):
- Username: your_username
- Password: your_password

**Features:**
- Real-time bidirectional WebSocket chat
- Login with your configured chat credentials
- 30-minute session timeout with auto-logout
- Logout button in top-right corner
- Alien aesthetic (phosphor green, scanlines)
- Historical trend analysis (CPU, memory, failures)

**Example queries:**
```
You:  "What's the overall system health?"
Bot:  "All services are healthy. Docker at 0.5% CPU, nordvpn at 0.2%..."

You:  "Why is system_backup failing?"
Bot:  "Analyzing logs... The backup process timed out due to disk space..."

You:  "What about CPU usage in the last 30 days?"
Bot:  "CPU trends are stable. Nordvpn averages 0.2%, docker 0.0%..."

You:  "Restart docker"
Bot:  "I can help with that. Execute: systemctl restart docker (y/n)?"
```

### 2. Interactive CLI Chat

**Start the interactive CLI:**
```bash
cd /home/heverz/py_projects/monit-intel
pixi run hello-mother
```

**Usage:**
```bash
> What is the current system status?
Agent: All services are currently healthy...

> Why is nordvpn_reconnect failing?
Agent: The service appears to have connection issues...

> Show me the failure history
Agent: Based on the past 30 days...

> exit
Goodbye!
```

**This CLI:**
- Connects directly to the agent
- Maintains conversation history
- Auto-detects OS for OS-specific commands
- Supports multi-line queries
- No browser needed

### 3. REST API (Programmatic)

**Test the API (use your configured chat credentials):**
```bash
# Check agent health
curl -u your_username:your_password http://localhost:8000/health
# â†’ {"status": "healthy", "database": "connected", "snapshots": 150}

# Get all service statuses
curl -u your_username:your_password http://localhost:8000/status | jq
# â†’ [{"service": "docker", "status": 0}, ...]

# Query via REST endpoint
curl -X POST http://localhost:8000/mother/chat \
  -u your_username:your_password \
  -H "Content-Type: application/json" \
  -d '{"query": "What about CPU usage?"}'
# â†’ {"response": "CPU usage is stable...", "timestamp": "..."}

# View conversation history
curl -u your_username:your_password "http://localhost:8000/mother/history?limit=10" | jq
```

**Available Endpoints:**
| Method | Endpoint | Purpose | Auth |
|--------|----------|---------|------|
| `GET` | `/health` | Agent status | Basic |
| `GET` | `/status` | All services | Basic |
| `POST` | `/mother/chat` | Chat query | Basic |
| `GET` | `/mother/history` | Chat history | Basic |
| `POST` | `/mother/actions/suggest` | Preview action | Basic |
| `POST` | `/mother/actions/execute` | Execute action | Basic |
| `GET` | `/mother/actions/audit` | Action audit log | Basic |

### 4. Manual Background Monitoring

**One-time ingestion run:**
```bash
pixi run ingest
```

**Continuous daemon (polls every 5 min):**
```bash
MONIT_USER=admin MONIT_PASS=your_monit_password pixi run agent &
```

**View console output:**
```bash
# Logs appear as:
[2026-01-03 20:53:38] detect_failures: Found 1 NEW failure
[2026-01-03 20:53:40] fetch_logs: Retrieving logs for system_backup
[2026-01-03 20:53:45] analyze_with_llm: Llama 3.1 analysis complete
  â†’ Root cause: Disk space exhausted (/data/tank at 98%)
  â†’ Recommendation: Expand storage or delete old backups
```

### Restart the Agent

**Development:**
```bash
# Kill old process
pkill -f "pixi run agent"

# Start new one
MONIT_USER=admin MONIT_PASS=your_monit_password pixi run agent
```

**Production (Systemd):**
```bash
sudo systemctl restart monit-intel-agent.service
sudo systemctl status monit-intel-agent.service
```

### Stop the Agent

**Development:**
```bash
pkill -f "pixi run agent"
```

**Production:**
```bash
sudo systemctl stop monit-intel-agent.service
sudo systemctl disable monit-intel-agent.service  # Don't auto-start
```

---

## ğŸ—ï¸ Architecture

**Service Health Query (REST API):**
```bash
curl -X POST http://localhost:8000/mother/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "Are there any service failures right now?"}'

# Response:
# {
#   "query": "Are there any service failures right now?",
#   "response": "No, all services are currently healthy. There are no service failures reported at this time...",
#   "timestamp": "2026-01-03T13:32:13.347162"
# }
```

**Historical Trend Query (30-day analysis with CPU/Memory metrics):**
```bash
curl -X POST http://localhost:8000/mother/chat \
  -u your_username:your_password \
  -H "Content-Type: application/json" \
  -d '{"query": "What about CPU usage in the last 30 days?"}'

# Response:
# {
#   "query": "What about CPU usage in the last 30 days?",
#   "response": "Based on the historical trend data, here are CPU usage observations:
#   - docker: avg 0.0%, min 0.0%, max 0.0%
#   - nordvpnd: avg 0.2%, min 0.2%, max 0.2%
#   - tailscaled: avg 0.1%, min 0.0%, max 0.1%
#   - Memory usage: docker 101.3 MB (0.1%), nordvpnd 119.0 MB (0.1%)
#   All services show minimal CPU/Memory consumption with stable trends.",
#   "timestamp": "2026-01-03T13:45:22.123456"
# }
```

**Via Web UI (WebSocket):**
```
User: "Why is docker unhealthy?"
Mother: "Based on the current system context, there are no immediate concerns about Docker's health. 
However, I'd like to highlight a few potential future issues: [Outdated Docker version], 
[Insufficient disk space], [Resource constraints]..."

User: "What should I monitor for sshd?"
Mother: "For SSH security and stability, monitor: Failed login attempts, Connection rate limits, 
Resource usage, Configuration changes, Certificate/key validity..."

User: "Did we have failures recently?"
Mother: "Yes. Based on the 30-day historical data, the alpha service has failed 5 times (100% failure rate). 
All other services remain healthy. I recommend investigating the root cause..."

User: "What's the overall system health?"
Mother: "All services are currently healthy: docker âœ“, sshd âœ“, zfs-zed âœ“. 
The system shows no critical issues at this moment. Continue routine monitoring..."
```

### WebSocket Message Format

The chat UI communicates with the agent via WebSocket at `/ws/chat`. Message types:

**User Messages:**
```json
{
  "type": "message",
  "content": "user query here"
}

{
  "type": "action",
  "action": "restart",
  "service": "docker"
}

{
  "type": "action_confirm",
  "action": "restart",
  "service": "docker"
}
```

**Agent Responses:**
```json
{
  "type": "thinking",
  "message": "Processing your query..."
}

{
  "type": "response",
  "content": "analysis or answer",
  "timestamp": "2026-01-03T12:00:00"
}

{
  "type": "action_suggestion",
  "action": "restart",
  "service": "docker",
  "command": "systemctl restart docker",
  "description": "Restart the docker service"
}

{
  "type": "action_result",
  "success": true,
  "exit_code": 0,
  "output": "command output here",
  "timestamp": "2026-01-03T12:00:00"
}
```

## ğŸ—ï¸ Architecture

### Components Overview

| Module | Purpose |
|--------|---------|
| `src/monit_intel/ingest.py` | Polls Monit XML API every 5 min, stores snapshots, cleans old data |
| `src/monit_intel/main.py` | Daemon runner - checks for failures every 5 min |
| `src/monit_intel/hello_mother.py` | Interactive CLI chat interface |
| `src/monit_intel/agent/api.py` | FastAPI REST + WebSocket server |
| `src/monit_intel/agent/graph.py` | LangGraph workflow definition (DAG compilation) |
| `src/monit_intel/agent/state.py` | LangGraph state definition |
| `src/monit_intel/agent/nodes.py` | Individual workflow nodes (database, log fetching, LLM) |
| `src/monit_intel/agent/mother.py` | Interactive chat manager with context injection |
| `src/monit_intel/agent/actions.py` | Safe command executor with whitelist and audit logging |
| `src/monit_intel/agent/static/chat.html` | Web chat UI (HTML/CSS/JavaScript) |
| `src/monit_intel/tools/log_reader.py` | Hybrid log reader (files, journalctl, glob patterns) |

### How It Works: Two Parallel Systems

The system has **two distinct workflows** running simultaneously:

#### **System 1: Background Agent (LangGraph Daemon)**

Runs automatically every 5 minutes in the background, detecting and analyzing failures.

```
START
  â†“
detect_failures() [Node 1]
  â”œâ”€ Query SQLite snapshots for failures (status != 0)
  â”œâ”€ Check failure_history: Is this NEW or CHANGED?
  â”œâ”€ Set is_critical=True only for NEW/CHANGED failures
  â””â”€ Skip unchanged failures (save GPU compute)
  â†“
fetch_logs_and_context() [Node 2]
  â”œâ”€ Extract failed service names
  â”œâ”€ Use LogReader to fetch logs from:
  â”‚  â”œâ”€ Log files (tail strategy: /var/log/service.log)
  â”‚  â”œâ”€ Glob patterns (newest_file: /path/logs_*.log)
  â”‚  â””â”€ Journalctl (journalctl -u service.service)
  â”œâ”€ Apply per-service max_lines limits (50-150 lines)
  â””â”€ Append logs to context
  â†“
analyze_with_llm() [Node 3]
  â”œâ”€ Check: is_critical == True?
  â”œâ”€ If False: Skip LLM (unchanged failure)
  â”œâ”€ If True: Send to Llama 3.1 with context
  â”œâ”€ Llama analyzes logs + service status
  â””â”€ Return root cause analysis
  â†“
END (sleep 5 min, repeat)
```

**Smart Logic: is_critical Flag**
- **NEW Failure:** Service was healthy, now failed â†’ `is_critical=True` â†’ **Analyze**
- **ONGOING:** Service still failing, same status â†’ `is_critical=False` â†’ **Skip** (don't re-analyze)
- **CHANGED:** Service status changed â†’ `is_critical=True` â†’ **Analyze**
- **RECOVERED:** Service back to healthy â†’ `is_critical=False` â†’ **Done**

This saves significant GPU compute by not re-analyzing the same failure repeatedly.

#### **System 2: Interactive Chat (Mother / MU/TH/UR)**

A user-facing chat interface that queries the LLM on-demand when you ask questions.

```
User Types Message in Browser
  â†“
WebSocket â†’ FastAPI /ws/chat endpoint
  â†“
Mother.query_agent() [agent/mother.py]
  â”œâ”€ Extract mentioned services from user query
  â”œâ”€ Gather context:
  â”‚  â”œâ”€ Current service statuses
  â”‚  â”œâ”€ Historical trends (30-day data with CPU/memory metrics)
  â”‚  â”œâ”€ Recent logs for relevant services
  â”‚  â””â”€ System info (OS, package manager, hostname)
  â”œâ”€ Detect system: Ubuntu? Fedora? macOS?
  â””â”€ Inject context into LLM prompt
  â†“
Llama 3.1 Processes Query + Context
  â”œâ”€ Understands OS-specific commands
  â”œâ”€ Answers with service history
  â””â”€ Provides actionable recommendations
  â†“
Response Streamed Back via WebSocket
  â†“
Browser Displays in Chat UI
```

**Key Differences:**

| Aspect | Background Agent | Mother Chat |
|--------|---|---|
| **Trigger** | Runs every 5 min automatically | User sends message |
| **Purpose** | Detect failures proactively | Answer user questions |
| **Process** | Multi-node workflow (Graph) | Single LLM call |
| **Output** | Console logs | Conversational response |
| **Context** | Current failure data | Historical + current data |

### Historical Data: 30-Day Snapshots

Every 5 minutes, `ingest.py` stores a complete service snapshot in SQLite:

```json
{
  "timestamp": "2026-01-03T20:53:38",
  "service_name": "docker",
  "status": 0,  // 0=healthy, other=failed
  "raw_json": {
    "cpu": { "percent": "0.5", "percenttotal": "0.5" },
    "memory": { "percent": "0.1", "kilobyte": "103752" },
    "uptime": "517935",
    "threads": "33",
    ...full Monit data...
  }
}
```

The `get_historical_trends()` function extracts this data:
- **CPU metrics:** Min/max/average over 30 days per service
- **Memory usage:** Current and historical percentages
- **Failure rates:** How often did this service fail?
- **Status trends:** Service health over time

When you ask "What about CPU usage?", the Mother chat:
1. Queries the snapshots table for 30 days of data
2. Extracts CPU percentages for each service
3. Calculates trends (nordvpnd avg 0.2%, docker avg 0.0%, etc.)
4. Passes to LLM with actual numbers
5. Returns analysis: "CPU usage is stable. Nordvpnd is the top consumer..."

### System Context Injection

When you chat with Mother, the system automatically:

1. **Detects your OS:**
   ```python
   if "Ubuntu" in lsb_release:
       package_manager = "apt"
   elif "Fedora" in lsb_release:
       package_manager = "dnf"
   # etc.
   ```

2. **Injects into LLM prompt:**
   ```
   "You are MU/TH/UR running on Ubuntu 24.04 (beta-boy)
    Package manager: apt (not dnf, zypper, or pacman)
    When suggesting package installs, use: sudo apt install <package>
    For service management, use: systemctl ...
    Current hostname: beta-boy"
   ```

3. **Result:** LLM gives OS-specific advice automatically
   - Ask on Ubuntu â†’ get `apt` commands
   - Ask on Fedora â†’ get `dnf` commands
   - No manual context needed!

### Per-Service Log Configuration

The log registry tells LogReader where to find logs:

```python
log_registry = {
    "system_backup": {
        "strategy": "newest_file",           # Pick latest file matching pattern
        "pattern": "/data/tank/backups/sys_restore/backup_log_*.log",
        "max_lines": 150                     # Verbose backup logs
    },
    "nordvpn_reconnect": {
        "strategy": "tail_file",             # Tail single log file
        "path": "/var/log/nordvpn-reconnect.log",
        "max_lines": 75
    },
    "nordvpn_status": {
        "strategy": "journalctl",            # Query systemd journal
        "unit": "nordvpnd.service",
        "max_lines": 50                      # Terse service logs
    }
}
```

**Why per-service limits?**
- Backup logs are verbose (need 150 lines to see full context)
- Service status is terse (50 lines usually sufficient)
- Prevents VRAM overflow on GPU with long contexts
- Keeps LLM inference fast (2-5 seconds per analysis)

### Data Flow Diagram

```
Monit XML API (every 5 min)
     â†“
[ingest.py] 
     â”œâ†’ INSERT snapshots with full raw_json
     â”œâ†’ UPDATE failure_history (track NEW/ONGOING/CHANGED)
     â””â†’ DELETE snapshots >30 days old
     â†“
[SQLite: snapshots + failure_history + conversations]
     â”œâ”€ 30 days Ã— ~30 services Ã— 12 checks/day = ~10K snapshots
     â””â”€ Size: ~20-25MB
     â†“
     â”œâ”€ BRANCH 1: Background Agent (every 5 min)
     â”‚  â”œâ†’ detect_failures: Query snapshots, check failure_history
     â”‚  â”œâ†’ is_critical? If No â†’ Skip. If Yes â†’ Continue.
     â”‚  â”œâ†’ fetch_logs_and_context: Use LogReader + registry
     â”‚  â””â†’ analyze_with_llm: Send to Llama 3.1 (GPU inference)
     â”‚     â””â†’ Console output: Root cause analysis
     â”‚
     â””â”€ BRANCH 2: Interactive Chat (on user message)
        â”œâ†’ Mother receives query via WebSocket
        â”œâ†’ get_historical_trends(): Extract CPU/memory/failure data
        â”œâ†’ detect OS, inject system context
        â”œâ†’ Send to Llama 3.1 with enriched context
        â””â†’ Stream response back to browser
```

### Full System Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MONIT-INTEL AGENT SYSTEM                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        BACKEND (Server)                            â•‘
â•‘                                                                    â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â•‘
â•‘  â”‚  Monit Server    â”‚  Runs on localhost:2812                     â•‘
â•‘  â”‚  (beta-boy)      â”‚  Monitors 30+ services                      â•‘
â•‘  â”‚  Port 2812       â”‚  Exposes XML status API                     â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â•‘
â•‘           â”‚ GET /_status?format=xml                               â•‘
â•‘           â–¼                                                        â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â•‘
â•‘  â”‚   ingest.py (Scheduler via systemd)      â”‚                    â•‘
â•‘  â”‚   - Runs every 5 minutes                 â”‚                    â•‘
â•‘  â”‚   - Polls Monit XML                      â”‚                    â•‘
â•‘  â”‚   - Parses service status + metrics      â”‚                    â•‘
â•‘  â”‚   - Stores complete snapshot raw_json    â”‚                    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â•‘
â•‘               â”‚ INSERT snapshots, UPDATE failure_history          â•‘
â•‘               â–¼                                                    â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â•‘
â•‘  â”‚   SQLite: monit_history.db               â”‚                    â•‘
â•‘  â”‚   - snapshots (30-day rolling)           â”‚                    â•‘
â•‘  â”‚   - failure_history (state tracking)     â”‚                    â•‘
â•‘  â”‚   - conversations (chat history)         â”‚                    â•‘
â•‘  â”‚   - action_audit_log (executed commands) â”‚                    â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â•‘
â•‘               â”‚                                                    â•‘
â•‘     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â•‘
â•‘     â”‚                      â”‚                                      â•‘
â•‘     â–¼ (every 5 min)        â–¼ (on user message)                   â•‘
â•‘                                                                    â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â•‘
â•‘  â”‚  BACKGROUND AGENT  â”‚  â”‚   MOTHER (Chat)      â”‚                â•‘
â•‘  â”‚  (main.py daemon)  â”‚  â”‚   (agent/mother.py)  â”‚                â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â•‘
â•‘           â”‚                        â”‚                              â•‘
â•‘           â–¼                        â–¼                              â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â•‘
â•‘  â”‚   LangGraph Workflow (agent/graph.py)            â”‚             â•‘
â•‘  â”‚                                                  â”‚             â•‘
â•‘  â”‚   [1] detect_failures()                          â”‚             â•‘
â•‘  â”‚       â””â”€ Query: status != 0                      â”‚             â•‘
â•‘  â”‚       â””â”€ Check: NEW or CHANGED?                  â”‚             â•‘
â•‘  â”‚       â””â”€ Set is_critical flag                    â”‚             â•‘
â•‘  â”‚           â–¼                                      â”‚             â•‘
â•‘  â”‚   [2] fetch_logs_and_context()                   â”‚             â•‘
â•‘  â”‚       â””â”€ Use LogReader + registry               â”‚             â•‘
â•‘  â”‚       â””â”€ tail_file: /var/log/service.log        â”‚             â•‘
â•‘  â”‚       â””â”€ newest_file: glob patterns             â”‚             â•‘
â•‘  â”‚       â””â”€ journalctl: systemd units              â”‚             â•‘
â•‘  â”‚       â””â”€ Apply max_lines per service            â”‚             â•‘
â•‘  â”‚           â–¼                                      â”‚             â•‘
â•‘  â”‚   [3] analyze_with_llm()                         â”‚             â•‘
â•‘  â”‚       â””â”€ if NOT is_critical: SKIP               â”‚             â•‘
â•‘  â”‚       â””â”€ if is_critical:                         â”‚             â•‘
â•‘  â”‚           â”œâ”€ Send logs + context to Llama 3.1   â”‚             â•‘
â•‘  â”‚           â””â”€ Return root cause analysis         â”‚             â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â•‘
â•‘           â”‚                        â”‚                              â•‘
â•‘           â–¼                        â–¼                              â•‘
â•‘  Console output              Stream response                      â•‘
â•‘  (root cause analysis)       via WebSocket                        â•‘
â•‘                                                                    â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â•‘
â•‘  â”‚   System Context Injection                       â”‚             â•‘
â•‘  â”‚   - Detect OS (Ubuntu, Fedora, Arch, etc.)     â”‚             â•‘
â•‘  â”‚   - Find package manager (apt, dnf, zypper)   â”‚             â•‘
â•‘  â”‚   - Get hostname, distro version               â”‚             â•‘
â•‘  â”‚   - Build system-aware LLM prompt              â”‚             â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â•‘
â•‘           â”‚                                                       â•‘
â•‘           â–¼                                                       â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â•‘
â•‘  â”‚   Ollama (GPU - RTX 4000)                        â”‚             â•‘
â•‘  â”‚   Llama 3.1:8b                                   â”‚             â•‘
â•‘  â”‚   - Inference time: 2-5 seconds                 â”‚             â•‘
â•‘  â”‚   - Understands OS-specific context             â”‚             â•‘
â•‘  â”‚   - Returns intelligent analysis                â”‚             â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â•‘
â•‘           â”‚                                                       â•‘
â•‘           â–¼                                                       â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â•‘
â•‘  â”‚   FastAPI Server (agent/api.py)                 â”‚             â•‘
â•‘  â”‚   - Port 8000                                    â”‚             â•‘
â•‘  â”‚   - HTTP Basic Auth (Monit credentials)         â”‚             â•‘
â•‘  â”‚   - WebSocket: /ws/chat                         â”‚             â•‘
â•‘  â”‚   - REST endpoints: /health, /status, etc.     â”‚             â•‘
â•‘  â”‚   - Message-based WebSocket auth               â”‚             â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â•‘
â•‘                                                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      FRONTEND (Browser)                            â•‘
â•‘                                                                    â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â•‘
â•‘  â”‚   MU/TH/UR Chat UI (http://localhost:8000/chat)â”‚             â•‘
â•‘  â”‚                                                  â”‚             â•‘
â•‘  â”‚   [Login Overlay]                                â”‚             â•‘
â•‘  â”‚   â”œâ”€ Username: admin                             â”‚             â•‘
â•‘  â”‚   â””â”€ Password: monit                             â”‚             â•‘
â•‘  â”‚       â”‚ (localStorage cached for 30 min)        â”‚             â•‘
â•‘  â”‚       â–¼                                          â”‚             â•‘
â•‘  â”‚   [Main Chat Interface]                          â”‚             â•‘
â•‘  â”‚   â”œâ”€ Alien aesthetic (phosphor green)           â”‚             â•‘
â•‘  â”‚   â”œâ”€ CRT scanlines effect                       â”‚             â•‘
â•‘  â”‚   â”œâ”€ Message history display                    â”‚             â•‘
â•‘  â”‚   â”œâ”€ Input field for user queries               â”‚             â•‘
â•‘  â”‚   â””â”€ LOGOUT button (top-right)                  â”‚             â•‘
â•‘  â”‚       â””â”€ 30-min timeout triggers auto-logout    â”‚             â•‘
â•‘  â”‚       â””â”€ Activity resets timeout                â”‚             â•‘
â•‘  â”‚                                                  â”‚             â•‘
â•‘  â”‚   [WebSocket Connection]                         â”‚             â•‘
â•‘  â”‚   â”œâ”€ Initial: First message contains auth       â”‚             â•‘
â•‘  â”‚   â”œâ”€ Ongoing: "type": "message", "content": ... â”‚             â•‘
â•‘  â”‚   â””â”€ Auto-reconnect on disconnect               â”‚             â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â•‘
â•‘                                                                    â•‘
â•‘  User Query Flow:                                                  â•‘
â•‘  â”Œâ”€ User: "What about CPU usage?"                  â•‘             â•‘
â•‘  â”‚  â–¼                                               â•‘             â•‘
â•‘  â”‚  Browser sends JSON via WebSocket               â•‘             â•‘
â•‘  â”‚  â–¼                                               â•‘             â•‘
â•‘  â”‚  Mother class receives query                    â•‘             â•‘
â•‘  â”‚  â–¼                                               â•‘             â•‘
â•‘  â”‚  get_historical_trends() fetches 30 days data  â•‘             â•‘
â•‘  â”‚  â”œâ”€ CPU metrics: min/avg/max per service       â•‘             â•‘
â•‘  â”‚  â”œâ”€ Memory usage: docker 101MB, nordvpn 119MB  â•‘             â•‘
â•‘  â”‚  â””â”€ Status: "docker HEALTHY", "alpha FAILED"   â•‘             â•‘
â•‘  â”‚  â–¼                                               â•‘             â•‘
â•‘  â”‚  LLM gets enriched prompt with actual data     â•‘             â•‘
â•‘  â”‚  â–¼                                               â•‘             â•‘
â•‘  â”‚  Llama 3.1: "CPU is stable. nordvpnd at 0.2%"â”‚             â•‘
â•‘  â”‚  â–¼                                               â•‘             â•‘
â•‘  â”‚  Response streams via WebSocket                 â•‘             â•‘
â•‘  â”‚  â–¼                                               â•‘             â•‘
â•‘  â”‚  Browser displays: "CPU usage analysis..."     â•‘             â•‘
â•‘  â””â”€ Done                                            â•‘             â•‘
â•‘                                                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```
                 â”‚ If NEW/CHANGED
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   agent/graph.py (LangGraph DAG)        â”‚
    â”‚   â””â”€ fetch_logs_and_context()           â”‚
    â”‚      Use Log Registry                   â”‚
    â”‚      (per-service max_lines)            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚
        â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Log Files  â”‚   â”‚ Journalctl   â”‚
    â”‚ (tail)     â”‚   â”‚ (systemd)    â”‚
    â”‚ (glob)     â”‚   â”‚              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Log content (per-service lines)
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   agent/graph.py                        â”‚
    â”‚   â””â”€ analyze_with_llm()                 â”‚
    â”‚      Llama 3.1:8b Analysis              â”‚
    â”‚      (2-5 sec inference)                â”‚
    â”‚      (Only runs for NEW/CHANGED)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Console Output                        â”‚
    â”‚   - Root cause analysis                 â”‚
    â”‚   - Suggested remediation               â”‚
    â”‚   - State tracking (NEW vs ONGOING)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸŒ REST API

Once the agent is running (via systemd or manually with `--api`), the REST API is available on `localhost:8000`.

### Endpoints

| Method | Endpoint | Purpose | Example |
|--------|----------|---------|---------|
| `GET` | `/` | API info & endpoints | `curl http://localhost:8000/` |
| `GET` | `/health` | DB status & snapshot count | `curl http://localhost:8000/health` |
| `GET` | `/status` | All services + last_checked | `curl http://localhost:8000/status` |
| `POST` | `/analyze` | Trigger analysis | `curl -X POST http://localhost:8000/analyze` |
| `GET` | `/history?service=X&days=7` | Failure history for service | `curl "http://localhost:8000/history?service=system_backup&days=7"` |
| `GET` | `/logs/{service}` | Latest logs for service | `curl http://localhost:8000/logs/nordvpn_status` |

### Examples

```bash
# Check if agent is healthy
curl http://localhost:8000/health
# {"status": "healthy", "database": "connected", "snapshots": 150}

# Get all service statuses
curl http://localhost:8000/status | jq '.[] | {service: .service, status: .status}'

# Trigger analysis (returns LLM output if failures detected)
curl -X POST http://localhost:8000/analyze

# Query failure history for last 7 days
curl "http://localhost:8000/history?service=system_backup&days=7"

# Get latest logs for a service
curl http://localhost:8000/logs/nordvpn_status | jq '.logs' | head -20
```

## ğŸ‘©â€ğŸ’» Mother: Interactive Chat Interface (Phase 6)

**Mother** is an interactive chat interface that lets you query the agent in natural language. It automatically injects service context and failure history into the LLM prompt.

### Mother REST API Endpoints

| Method | Endpoint | Purpose | Example |
|--------|----------|---------|---------|
| `POST` | `/mother/chat` | Chat with agent | `curl -X POST http://localhost:8000/mother/chat -H "Content-Type: application/json" -d '{"query": "Why is system_backup failing?"}'` |
| `GET` | `/mother/history` | View conversations | `curl http://localhost:8000/mother/history?limit=10` |
| `DELETE` | `/mother/clear` | Clear chat history | `curl -X DELETE http://localhost:8000/mother/clear` |

### Mother CLI

Use the interactive Mother CLI for a better UX:

```bash
# Chat with the agent
pixi run python mother-cli.py chat "Why is nordvpn_status failing?"

# View conversation history
pixi run python mother-cli.py history --limit 20

# Clear all conversations
pixi run python mother-cli.py clear

# Interactive mode (type 'help' for commands)
pixi run python mother-cli.py interactive
```

### How Mother Works

1. **Context Injection**: Extracts mentioned services from your query
2. **Status Lookup**: Fetches current service status and failure history
3. **Log Retrieval**: Uses LogReader to fetch relevant logs
4. **LLM Analysis**: Passes enriched context to Llama 3.1 for analysis
5. **Conversation Persistence**: Stores all chats in SQLite for history

**Example conversation:**
```
You: "Why is system_backup failing?"

Agent: [Analyzes current system_backup status, fetches last 150 lines of logs, 
        queries failure history over last 7 days, and provides root cause analysis]

Agent Response: "system_backup has failed 3 times in the last 7 days. The most 
recent failure shows disk space exhaustion in /data/tank. The backup_log 
indicates the backup process timed out after 4 hours when trying to sync 
2.5TB of data. Recommendation: Expand storage or increase timeout threshold."
```

---

## ğŸ› ï¸ Actions: Safe Command Execution (Phase 7)

**Actions** allow the agent to suggest AND execute safe system commands for remediation. All actions are whitelisted, require user approval, and logged for audit.

### Safe Actions Whitelist

| Action | Command | Use Case |
|--------|---------|----------|
| `systemctl_restart` | `systemctl restart <service>` | Recover from transient failures |
| `systemctl_stop` | `systemctl stop <service>` | Prevent cascading failures |
| `systemctl_start` | `systemctl start <service>` | Bring service online |
| `systemctl_status` | `systemctl status <service>` | Get detailed systemd state |
| `monit_monitor` | `sudo monit monitor <service>` | Force Monit re-check |
| `monit_start` | `sudo monit start <service>` | Tell Monit to bring online |
| `monit_stop` | `sudo monit stop <service>` | Tell Monit to stop watching |
| `journalctl_view` | `journalctl -u <service> -n 50` | View service logs |

### Actions REST API Endpoints

| Method | Endpoint | Purpose | Example |
|--------|----------|---------|---------|
| `POST` | `/mother/actions/suggest` | Preview action without executing | `curl -X POST http://localhost:8000/mother/actions/suggest -d '{"action": "systemctl_restart", "service": "nordvpnd"}'` |
| `POST` | `/mother/actions/execute` | Execute action with approval | `curl -X POST http://localhost:8000/mother/actions/execute -d '{"action": "systemctl_status", "service": "nordvpnd", "approve": true}'` |
| `GET` | `/mother/actions/audit` | View action audit log | `curl http://localhost:8000/mother/actions/audit?limit=50` |

### Actions CLI

```bash
# Suggest an action (preview only, doesn't execute)
pixi run python mother-cli.py actions suggest systemctl_restart nordvpnd

# Execute with approval
pixi run python mother-cli.py actions execute systemctl_restart nordvpnd --approve

# View audit log (all executed actions)
pixi run python mother-cli.py actions audit --limit 50
```

### Execution Flow

```
1. Agent detects failure
2. Agent suggests action: "Consider restarting nordvpnd"
3. User confirms via CLI: --approve flag
4. Action is whitelisted and approved
5. Command executes (e.g., systemctl restart nordvpnd)
6. Result logged to audit_audit_log table in SQLite
7. User sees output + confirmation
```

---

## âš™ï¸ Configuration

### Customize Service Log Registry

Edit `src/monit_intel/tools/log_reader.py` to add or modify how logs are fetched:

```python
log_registry = {
    "my_new_service": {
        "strategy": "tail_file",              # Options: tail_file, newest_file, journalctl
        "path": "/var/log/my_service.log",   # For tail_file
        "pattern": "/var/log/my_service_*.log",  # For newest_file (glob)
        "unit": "my-service.service",         # For journalctl
        "max_lines": 100                      # Context window size
    }
}
```

**Strategies:**
- `tail_file`: Read last N lines from a single file
- `newest_file`: Find newest file matching glob pattern, then tail
- `journalctl`: Query systemd journal for a specific unit

### Customize LLM System Prompt

Edit `src/monit_intel/agent/mother.py` in the `query_agent()` method:

```python
system_prompt = f"""You are MU/TH/UR, an expert system administrator.
You are running on {self.system_info['distro']} ({self.system_info['os']}).
Package manager: {self.system_info['package_manager']}
Hostname: {self.system_info['hostname']}

GUIDELINES:
- Provide OS-specific commands only
- Analyze failures with real data
- Never suggest destructive operations without explicit approval
- Include relevant logs in analysis
- Be concise and actionable
"""
```

### Adjust Monitoring Intervals

Edit `src/monit_intel/main.py`:

```python
# Change from 5 minutes to custom interval (in seconds)
MONITOR_INTERVAL = 300  # 5 minutes
```

Edit systemd timer for ingest (production):
```bash
sudo systemctl edit monit-intel-ingest.timer
# Modify: OnBootSec=5min, OnUnitActiveSec=5min
```

### Session Timeout Configuration

Edit `src/monit_intel/agent/static/chat.html`:

```javascript
// Current: 30 minutes
const SESSION_TIMEOUT = 1800000; // milliseconds

// Change to 1 hour:
const SESSION_TIMEOUT = 3600000;

// Change to 15 minutes:
const SESSION_TIMEOUT = 900000;
```

### Database Retention Policy

Edit `src/monit_intel/ingest.py`:

```python
# Current: Keep 30 days of snapshots
RETENTION_DAYS = 30

# Change to 14 days:
RETENTION_DAYS = 14

# Change to 60 days:
RETENTION_DAYS = 60
```

---

## ğŸ§  Features

### âœ… Hybrid State Management
- Tracks per-service failure history in SQLite
- Detects NEW vs ONGOING failures
- Skips LLM analysis for unchanged failures (saves GPU compute)
- Example: Service fails â†’ analyzed. Still failing 5 min later â†’ skipped

### âœ… 30-Day Data Retention
- Automatic cleanup after each ingestion
- Keeps database size ~20-25MB max
- Suitable for 30 days of history at 30 services / 5 min interval

### âœ… Configurable Per-Service Log Limits
Each service gets optimized context window:
- `system_backup`: 150 lines (verbose backups)
- `network_resurrect`: 100 lines (network operations)
- `gamma_conn`, `nordvpn_reconnect`: 75 lines (medium verbosity)
- `zfs_sanoid`: 100 lines (storage operations)
- `nordvpn_status`: 50 lines (terse service)

### âœ… Read-Only Analysis
- Agent reads logs but cannot execute destructive commands
- Safe for automated monitoring

## ğŸ“œ Log Registry

The agent automatically knows which logs to fetch for each service:

| Service | Strategy | Path/Unit | Max Lines |
|---------|----------|-----------|-----------|
| `system_backup` | Latest file | `/data/tank/backups/sys_restore/backup_log_*.log` | 150 |
| `nordvpn_reconnect` | Tail file | `/var/log/nordvpn-reconnect.log` | 75 |
| `nordvpn_status` | Journalctl | `nordvpnd.service` | 50 |
| `gamma_conn` | Journalctl | `tailscaled.service` | 75 |
| `network_resurrect` | Tail file | `/var/log/monit-network-restart.log` | 100 |
| `zfs_sanoid` | Journalctl | `sanoid.service` | 100 |

## ğŸ› ï¸ Configuration

### Extend the Log Registry

Edit `tools/log_reader.py`, function `get_logs_for_service()`:

```python
log_registry = {
    "my_service": {
        "strategy": "tail_file",           # or "newest_file", "journalctl"
        "path": "/path/to/logfile.log",   # for tail_file
        "pattern": "glob_pattern",         # for newest_file
        "unit": "service.service",         # for journalctl
        "max_lines": 75                    # customize per-service context
    }
}
```

### Customize Llama Prompt

Edit `agent/graph.py`, function `analyze_with_llm()`:

```python
system_prompt = """..."""  # Modify the system message here
```

## ğŸ“Š Database Schema

### snapshots (30-day rolling window)
```sql
CREATE TABLE snapshots (
    id INTEGER PRIMARY KEY,
    timestamp DATETIME,
    service_name TEXT,
    status INTEGER,       -- 0 = OK, other = failed
    raw_json TEXT        -- Full Monit service data
);
```

### failure_history (state tracking)
```sql
CREATE TABLE failure_history (
    service_name TEXT PRIMARY KEY,
    last_status INTEGER,
    last_checked DATETIME,
    times_failed INTEGER   -- How many times this service has failed
);
```

## ğŸ” Security

- âœ… **Chat passwords:** Hashed with PBKDF2-SHA256 (stored in SQLite, never plain text)
- âœ… **Monit credentials:** Stored in systemd env files (production, chmod 600) or .env (development, not in git)
- âœ… **HTTP Basic Auth:** All REST endpoints require valid chat credentials
- âœ… **WebSocket Auth:** Chat UI requires login with 30-minute session timeout
- âœ… **Read-only agent:** Cannot execute destructive commands (`rm`, `kill`)
- âœ… **Scoped logs:** Only reads paths specified in Log Registry
- âš ï¸ **No HTTPS:** Run behind reverse proxy (nginx) for production TLS

For detailed security architecture, see [SECURITY.md](SECURITY.md).

---

## ğŸ› Troubleshooting

### Monit Connection Fails

```bash
# Test connection (using Monit service password)
curl -u admin:your_monit_password http://localhost:2812/_status?format=xml | head -10

# Check Monit is running
sudo systemctl status monit

# Check Monit XML API is enabled
grep "set httpd" /etc/monit/monitrc
```

### Ollama Model Not Found

```bash
# List available models
ollama list

# Download Llama 3.1:8b
ollama pull llama3.1:8b

# Test Ollama is running
curl http://localhost:11434/api/tags
```

### Journal Access Denied

```bash
# Add your user to systemd-journal group
sudo usermod -aG systemd-journal $(whoami)

# Apply group changes
newgrp systemd-journal

# Verify access
journalctl -n 1
```

### Check Database State

```bash
pixi run python << 'EOF'
import sqlite3
conn = sqlite3.connect("monit_history.db")
cursor = conn.cursor()
cursor.execute("SELECT COUNT(*) FROM snapshots")
print(f"Total snapshots: {cursor.fetchone()[0]}")
conn.close()
EOF
```

### Systemd Service Won't Start

```bash
# Check detailed error
journalctl -u monit-intel-agent.service -n 50

# Verify service file syntax
systemd-analyze verify /etc/systemd/system/monit-intel-agent.service

# Check environment variables
cat /etc/systemd/system/monit-intel-agent.service.d/env.conf
```

### API Port 8000 Already in Use

```bash
# Find what's using the port
sudo lsof -i :8000

# Kill the old process
sudo kill -9 <PID>

# Or use pkill
pkill -f "pixi run agent"

# Restart service
sudo systemctl restart monit-intel-agent.service
```

### WebSocket Connection Fails

```bash
# Check agent is running (using chat credentials)
curl -u your_username:your_password http://localhost:8000/health

# Check WebSocket endpoint
# Open browser console and test:
# const ws = new WebSocket('ws://localhost:8000/ws/chat');
# ws.onopen = () => console.log('Connected!');
# ws.onerror = (e) => console.log('Error:', e);
```

### Agent Crashes Frequently

```bash
# Check logs for errors
journalctl -u monit-intel-agent.service -f

# Check available memory
free -h

# Check GPU VRAM
nvidia-smi

# If VRAM exhausted, reduce context window in log_reader.py
```

---

## ğŸ“ Next Steps & Future Enhancements

- [ ] Multi-host monitoring (extend to monitor multiple servers)
- [ ] Slack/Email alert escalation
- [ ] Grafana dashboard for historical trends
- [ ] Fine-tune Llama 3.1 model on server logs
- [ ] Predictive failure detection
- [ ] User role-based access control
- [ ] Integration with PagerDuty / Jira
